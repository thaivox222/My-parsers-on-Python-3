{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoC парсера сайта bestceramic.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#начинаем с импорта библиотек\n",
    "import requests\n",
    "from requests.auth import HTTPProxyAuth # для проксей\n",
    "from  bs4 import BeautifulSoup as bs\n",
    "import csv\n",
    "from urllib.request import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем глобальные параметры\n",
    "headers = {'accept': '*/*',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n",
    "          }\n",
    "domen = 'https://www.bestceramic.ru/'\n",
    "url='https://www.bestceramic.ru/catalog/'\n",
    "filename = 'product_links.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция get_category_links запускается первой. Она собирает ссылки на основные категории.\n",
    "Парсер работает по выбранной категории т.к. товарных SKU очень много и нет смысла мешать\n",
    "паркет со смесителями в один файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_category_links(headers, domen, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сбора ссылок на основные продуктовые категории\n",
    "def get_category_links(headers, domen, url):\n",
    "    session = requests.Session()\n",
    "    r = session.get(url, headers=headers)\n",
    "    soup = bs(r.text, 'lxml')\n",
    "    links = []\n",
    "    urls=[]\n",
    "    ul = soup.find('div', class_='tile-grid').find_all('a', class_='outside')\n",
    "    if ul:\n",
    "        for u in ul:\n",
    "             urls.append(u['href'])\n",
    "    for i in urls:\n",
    "        link  = urljoin(domen,i)\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как получили ссылки на товарные категории, выбираем любую для парсинга, например так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url - links[7] #выберет шестую по счету категорию для парсинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выбора товарной категории запускаем пагинатор и сборщик урлов целевых страниц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_plinks=[] #для вложенного списка\n",
    "product_links=[] #для прямого списка\n",
    "pagi_urls = get_pagination(url, headers) - #получаем список пагинационных урлов с одной категории\n",
    "for i in pagi_urls:\n",
    "    a = get_grid_links(i, headers)\n",
    "    temp_plinks.append(a)  \n",
    "\n",
    "for sublist in temp_plinks:\n",
    "    for item in sublist:\n",
    "        product_links.append(item)   \n",
    "save_products(product_links) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  блок сбора урлов пагинации\n",
    "def get_pagination(url, headers):\n",
    "    session = requests.Session()\n",
    "    r = session.get(url, headers=headers)\n",
    "    pagi=[] \n",
    "    pagi.append(url)\n",
    "    if r.status_code == 200:\n",
    "        soup = bs(r.content, 'lxml')\n",
    "        try:\n",
    "            count = int(soup.find('div', class_='tile-pages').contents[7].text)\n",
    "            for i in range(count+1):\n",
    "                u = str(url)+f'/{i}'\n",
    "                if u not in pagi:\n",
    "                    pagi.append(u)\n",
    "            del pagi[1:3] #удалим нулевую и первую пагинацию\n",
    "        except:            \n",
    "            pass\n",
    "        return pagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция сбора ссылок на целевые страницы.\n",
    "def get_grid_links(url, headers):\n",
    "    global domen\n",
    "    r = session.get(url, headers=headers)\n",
    "    soup = bs(r.content, 'lxml')\n",
    "    urls=[]\n",
    "    product_links=[]\n",
    "    if soup:\n",
    "        try:\n",
    "            a = soup.find_all('div',class_='grid-content-3col')\n",
    "            for i in a:\n",
    "                b = soup.find('div', class_='tile-grid').find_all('div', class_='title')\n",
    "                for i in b:\n",
    "                    c = i.find('a', href=True)\n",
    "                    urls.append(c['href'])\n",
    "        except:\n",
    "            print('Сбой на этапе чтения тегов')\n",
    "            pass\n",
    "    else:\n",
    "        print('Суп не обнаружен')\n",
    "        return False # это чтобы остановить работу функции. Т.к. если супа нет, то и склеивать ссылки смысла нет.\n",
    "    for i in urls:\n",
    "        link  = urljoin(domen,i)\n",
    "        product_links.append(link)\n",
    "    return product_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_products (result):\n",
    "    with open('product_links.csv', 'w') as outfile:\n",
    "        spamwriter = csv.writer(outfile, lineterminator='\\n')\n",
    "        spamwriter.writerow(['Product_Link']) \n",
    "        for i in result:\n",
    "            spamwriter.writerow([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После отработки в папке с парсером появится файл product_links.csv со ссылками.\n",
    "Имея сохраненные ссылки в файле нам не нужно запускать парсер каждый раз заново,\n",
    "в случаем сбоя или если сервер заблокирует бота. Сменив прокси, можно будет начать уже с этого шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И финально запускаем уже сам главный модуль такой командой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info(headers, domen, filename) # - запуск главного модуля парсера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все четко - появится сообщение - DONE! и в папке с парсером будет сохранен файл\n",
    "products.csv со всеми позициями из выбранной категории и их характеристиками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция парсинга целевых страниц\n",
    "def get_info(headers, domen, filename):\n",
    "    results = open_csv(filename)\n",
    "    products=[]\n",
    "    session = requests.Session()\n",
    "    for url in results:\n",
    "        params={} \n",
    "        r = session.get(url, headers=headers)\n",
    "        soup = bs(r.text, 'lxml')\n",
    "        if soup:\n",
    "            try:\n",
    "                a=soup.find('h1', class_='title')\n",
    "                if a:\n",
    "                    params['title']=a.text\n",
    "                b=soup.find('div', class_='model')\n",
    "                if b:\n",
    "                    params['articul']=b.text.rstrip('Сообщите код оператору').lstrip('Код: ')\n",
    "                c=soup.find('div', class_='product-body__title')\n",
    "                if c:\n",
    "                    params['price']=c.text.strip()\n",
    "                e=soup.find('div', class_='tab-inner__left-block').find_all('div', class_='prop-row')\n",
    "                if e:\n",
    "                    l1=[]\n",
    "                    l2=[]\n",
    "                    dict1={}\n",
    "                    for i in e:\n",
    "                        ee=i.find('div', class_='prop-title', span='').text.strip()\n",
    "                        l1.append(ee)\n",
    "                        eee=i.find('div', class_='prop-value', span='').text.strip()\n",
    "                        l2.append(eee)\n",
    "                    dict1 = dict(zip(l1, l2))\n",
    "                    params.update(dict1)\n",
    "                params['url']=url\n",
    "                f=soup.find('div', class_='big').find('a', href=True)['href']\n",
    "                if f:\n",
    "                    src = urljoin(domen,f)\n",
    "                    params['image']=src\n",
    "                products.append(params)\n",
    "            except:\n",
    "                print('Сбой при парсинге тегов')\n",
    "                pass\n",
    "        else:\n",
    "            print('Проблемы с супом')\n",
    "            return False\n",
    "    save_product_info(products)\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция загрузки файла со ссылками на целевые страницы\n",
    "def open_csv(filename):\n",
    "    results=[]\n",
    "    with open(filename, 'r') as data:\n",
    "        data.readline().rstrip()\n",
    "        for line in data:\n",
    "            results.append(line[:-1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сохранения результатов парсинга целевых страниц\n",
    "def save_product_info(products):\n",
    "    with open('products.csv', 'w', encoding = 'utf-8') as outfile:\n",
    "        spamwriter = csv.DictWriter(outfile, fieldnames=get_field_name(products), lineterminator='\\n')    \n",
    "        spamwriter.writeheader() #записывет заголовки столбцов равные ключам словаря\n",
    "        spamwriter.writerows(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сбора ключей словаря для формирования хэдера в CSV\n",
    "def get_field_name(products):\n",
    "    field_names = set()\n",
    "    for i in products:\n",
    "        for k in i:\n",
    "            field_names.add(k)\n",
    "    return field_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
