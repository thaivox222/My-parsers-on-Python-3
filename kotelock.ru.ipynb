{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoC парсера сайта kotelock.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#начинаем с импорта библиотек\n",
    "import requests\n",
    "from requests.auth import HTTPProxyAuth # для проксей\n",
    "from  bs4 import BeautifulSoup as bs\n",
    "import csv\n",
    "from urllib.request import urljoin\n",
    "#from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "#import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем глобальные параметры\n",
    "headers = {'accept': '*/*',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'\n",
    "          }\n",
    "domen = 'https://kotelock.ru/'\n",
    "url='https://kotelock.ru/collection/catalog'\n",
    "filename = 'kotelock_product_links.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск связки от сбора продуктовых категорий до сохранения ссылок на продуктовые страницы в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-444b82df9601>:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(0,count+1), desc='Getting pagination pages', leave=True): #experimental tooling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8b449487174e98bb1cac0298d98857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3058ab9459e44b72a6c106edfe679089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dff9f1a5f043dc99f8d162f22043b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14902cf574f408096bf56788449c057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dd68d6a1d448138ae9a128e12b6c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8c664f7cb34b258d1b25d7cffbdbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6eae5fbeec49edb3fbe558e5796ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eba6a8753d427d930b143c12607b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b784d98b19b4477aa22abffbddb49a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2214faf3db46438db9841a65c6c95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5a7c66cf8b46338042309b5be4f47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fa92c9e76e4e099b8fd5645516f061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf9555110cd4a21b9159db02859c359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183cafaca58c42828d5655ff9afab051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689d3303f9c64bc7bb9f9fe20d2434e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d960fcbdbdd4077af1d6753f8e03fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e0dc86590b474582f048e709610d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32425a4cd2944d5a82c1fafbc0e992f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a35246054a437b8b4f752adf9a3838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ca2a3742ab4d238cc188c82fb2ca20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0e1d1d73d24434b5abac40610c3620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1519d9ca42624d748c379d24bc400760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879efb7e9329493d840e99d0dd8f3608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04642e8c61d540f39f6e795f2d525a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting pagination pages'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "#формируем список продуктовых категорий и скармливаем его пагинатору\n",
    "temp_list=[]\n",
    "prodact_cat = get_category_links(headers, domen, url)\n",
    "for i in prodact_cat:\n",
    "    a = get_pagination(i, headers) #запуск пагинатора.на одной итерации обрабатывается одна категория\n",
    "    temp_list.append(a)\n",
    "#выпрямляем список пагинационных урлов    \n",
    "grid_links = []\n",
    "for sublist in temp_list:\n",
    "    for item in sublist:\n",
    "        grid_links.append(item)\n",
    "#сбор продуктовых урлов с грида        \n",
    "temp_plinks=[]\n",
    "for gl in grid_links:\n",
    "    gpl = get_product_links(gl, headers, domen)\n",
    "    temp_plinks.append(gpl)\n",
    "#выпрямляем список продуктовых урлов    \n",
    "product_links=[]\n",
    "for sublist in temp_plinks:\n",
    "    for item in sublist:\n",
    "        product_links.append(item)   \n",
    "save_products(product_links)\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сбора ссылок на целевые страницы\n",
    "def get_product_links(url, headers, domen):\n",
    "    session = requests.Session()\n",
    "    r = session.get(url, headers=headers)\n",
    "    soup = bs(r.text, 'lxml')\n",
    "    links = []\n",
    "    urls=[]\n",
    "    if url:\n",
    "        ul = soup.find('div', class_='main-product-block clearfix').find_all('a', href=True)\n",
    "        if ul:\n",
    "            for u in ul:\n",
    "                if str(u['href'])[0:11] == '/collection':\n",
    "                    continue\n",
    "                urls.append(u['href'])\n",
    "            for i in urls:\n",
    "                link  = urljoin(domen,i)\n",
    "                links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сбора ссылок на основные продуктовые категории\n",
    "def get_category_links(headers, domen, url):\n",
    "    session = requests.Session()\n",
    "    r = session.get(url, headers=headers)\n",
    "    soup = bs(r.text, 'lxml')\n",
    "    links = []\n",
    "    urls=[]\n",
    "    ul = soup.find_all('a', class_='category-block')\n",
    "    if ul:\n",
    "        for u in ul:\n",
    "             urls.append(u['href'])\n",
    "    for i in urls:\n",
    "        link  = urljoin(domen,i)\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  блок сбора урлов пагинации\n",
    "def get_pagination(url, headers):\n",
    "    session = requests.Session()\n",
    "    r = session.get(url, headers=headers)\n",
    "    pagi=[] \n",
    "    pagi.append(url)\n",
    "    if r.status_code == 200:\n",
    "        soup = bs(r.content, 'lxml')\n",
    "            count = int(soup.find('div', class_='pagination-list').find_all('a', title=True)[-1].text)\n",
    "            for i in tqdm(range(0,count+1), desc='Getting pagination pages', leave=True): #experimental tooling\n",
    "            #for i in range(count+1):\n",
    "                u = str(url)+f'?page={i}' #итератор перебора урлов в пагинации\n",
    "                if u not in pagi:\n",
    "                    pagi.append(u)\n",
    "            del pagi[1:3] #удалим нулевую и первую пагинацию\n",
    "        except:   \n",
    "            pass\n",
    "        return pagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сохранения файла со ссылками на целевые страницы\n",
    "def save_products (result):\n",
    "    with open(filename, 'w', encoding = 'utf-8') as outfile:\n",
    "        spamwriter = csv.writer(outfile, lineterminator='\\n')\n",
    "        spamwriter.writerow(['Product_Link']) \n",
    "        for i in result:\n",
    "            spamwriter.writerow([i])\n",
    "            \n",
    "# Функция загрузки файла со ссылками на целевые страницы\n",
    "def open_csv(filename):\n",
    "    results=[]\n",
    "    with open(filename, 'r') as data:\n",
    "        data.readline().rstrip()\n",
    "        for line in data:\n",
    "            results.append(line[:-1])\n",
    "    return results\n",
    "\n",
    "#Функция сохранения результатов парсинга целевых страниц\n",
    "def save_product_info(products):\n",
    "    with open('products.csv', 'w', encoding = 'utf-8') as outfile:\n",
    "        spamwriter = csv.DictWriter(outfile, fieldnames=get_fields_name(products), lineterminator='\\n')    \n",
    "        spamwriter.writeheader() #записывет заголовки столбцов равные ключам словаря\n",
    "        spamwriter.writerows(products)\n",
    "        \n",
    "# Вспом.функция для сбора названия полей с словарях с продуктами для формирования заголовка в CSV (Без сортировки)\n",
    "def get_field_name(products):\n",
    "    field_names = set()\n",
    "    for i in products:\n",
    "        for k in i:\n",
    "            field_names.add(k)\n",
    "    return field_names\n",
    "\n",
    "#версия 2 с сортированными названиями заголовков (текущая) Использовать её!\n",
    "def get_fields_name(products):\n",
    "    headerz=[]\n",
    "    field_names = set()\n",
    "    for i in products:\n",
    "        for k in i:\n",
    "            field_names.add(k)\n",
    "    for i in field_names:\n",
    "        if i=='1_title':\n",
    "            headerz.append(i)\n",
    "        if i=='2_articul':\n",
    "            headerz.append(i)\n",
    "        if i=='3_price':\n",
    "            headerz.append(i)\n",
    "        if i=='4_consistation':\n",
    "            headerz.append(i)\n",
    "        #if i=='6_volume':\n",
    "            #headerz.append(i)\n",
    "        if i=='5_weight/volume':\n",
    "            headerz.append(i)\n",
    "        if i=='7_manufacturer':\n",
    "            headerz.append(i)\n",
    "        if i=='8_stock':\n",
    "            headerz.append(i)\n",
    "        if i=='91_url':\n",
    "            headerz.append(i)\n",
    "        if i=='92_image':\n",
    "            headerz.append(i)\n",
    "        if i=='93_description':\n",
    "            headerz.append(i)\n",
    "        headerz.sort()            \n",
    "    return headerz\n",
    "\n",
    "#функция проверки уже спарсенных урлов (чтобы не запускать по ним парсинг повторно)\n",
    "def get_visited_urls():\n",
    "    try:\n",
    "        file = 'products.csv'\n",
    "        if file:\n",
    "            df = pd.read_csv(file, encoding = 'utf-8')\n",
    "            urls_done = df['91_url'].tolist()\n",
    "            visited = set(urls_done)\n",
    "            return visited, df\n",
    "    except FileNotFoundError:\n",
    "        df=pd.DataFrame()\n",
    "        visited=set()\n",
    "        return visited, df # если файла не существует, то вернет путой сет и список\n",
    "\n",
    "# Функция объединения прошлых и текущих результатов в 1 общий файл\n",
    "def add_previous_part(df):\n",
    "    if df.shape[0] != 0:\n",
    "        file = 'C:/Users/Vladimir/Coding/Notebooks/Парсинг/products.csv'\n",
    "        df.to_csv(file, mode='a', index=False, encoding = 'utf-8', header=False)\n",
    "    else:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Версия 1  - самая простая. Она просто сохраняет итоговый результат в файл. Как опцию,можно включить сохранение\n",
    "в SQLlite базу. Для этого надо раскоментить соотв. строчку. Главнй цикл реализован с помощью tdqm.\n",
    "\n",
    "Версия 2  - уже более сложная. В ней реализована функция промежуточного сохранения в случае,\n",
    "если на какой-то итерации происходит проблема с супом (в условии прописано if soup/else, хотя можно дописать\n",
    "что if soup else if soup = None soup/else. Т.е вроде суп есть, но он пустой)\n",
    "В связи с тем, что предполагается что у нас есть некие промежуточные результаты в версию 2 заложен механизм\n",
    "проверки уже спарсеных страниц и такие страницы повторно не парсятся. \n",
    "Новые результаты объеденяются со предыдущими результатами в том же файле.\n",
    "Есть некая особенность, если запустить 2 раза парсинг подряд, то файл products будет без хедера. но со всем содержимым.\n",
    "Это потому что  при аппенде датафрейма отключены хедеры.\n",
    "\n",
    "Версия 3 - такая же, как и версия 2 +  я пробовал обработать эксепшены, но получилось не очень. Сам код без ошибок.\n",
    "Но я тестировал просто врубая на ноуте самолетный режим, эмулируя экстренную потерю соединения. Я занес в эксепшн 2 ошибки, которые вылезли при вкл. самолета, но код их не обрабатывает. все равно выбивает на эксепшн.  Понятно, что если сервер\n",
    "будет банить, скорее всего будет какая-нб 400-я или 500-я ошибка. Еще в этой версии я отключил tdqm т.к. он выглядит крипово\n",
    "на while цикле с очередью (decue) - прогресс бар не работает\n",
    "Вместо него просто прописал строковый output +добавил итератор по циклу. Стало приятнее на глаз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обрабатываю ссылку № 1\n",
      "Обрабатываю ссылку № 2\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 3\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 4\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 5\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 6\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 7\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 8\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 9\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 10\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "Обрабатываю ссылку № 11\n",
      "Часть тегов не обнаружена. Двигаюсь дальше\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "get_info3(headers, filename)  # - запуск главного модуля парсера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВЕРСИЯ 1\n",
    "def get_info(headers, filename):\n",
    "    results = open_csv(filename)\n",
    "    products=[]\n",
    "    session = requests.Session()\n",
    "    #for url in results:\n",
    "    for url in tqdm(results, desc='Getting info from product page url', leave=False): #experimental tooling   \n",
    "        params={}\n",
    "        r = session.get(url, headers=headers)\n",
    "        soup = bs(r.text, 'lxml')\n",
    "        if soup:\n",
    "            try:\n",
    "                params['91_url']=url\n",
    "                j=soup.find('div', class_='my_recipe_image-box').find_next('img')['src']\n",
    "                if j:\n",
    "                    params['92_image']=j\n",
    "                a=soup.find('span', class_='product-title').text\n",
    "                if a:\n",
    "                    params['1_title']=a\n",
    "                i=soup.find('div', class_='product-desc', p='').text\n",
    "                if i: \n",
    "                    params['93_description']=i\n",
    "                b=soup.find('span', style='white-space:nowrap;').text\n",
    "                if b:\n",
    "                    params['2_articul']=b.strip()\n",
    "                c=soup.find('span', class_='this_product_price itemPrice').text.lstrip('Цена:').strip()\n",
    "                if c:\n",
    "                    params['3_price']=c \n",
    "                d=soup.find('span', class_='itemNal product-option-isset').text    \n",
    "                if d: \n",
    "                    params['8_stock']=d\n",
    "                e=soup.find('span', class_='itemLbl product-option-weight', span='').find_next_sibling('span').text.strip()\n",
    "                if e: \n",
    "                    params['5_weight/volume']=e\n",
    "                f=soup.find('div', class_='itemVal', span='').text.lstrip('Объем:').strip()\n",
    "                #if f: \n",
    "                    #params['6_volume/weight']=f\n",
    "               #g=soup.find('span', class_='itemLbl product-option-manufacturer', span='').find_next_sibling('span').text\n",
    "                if g: \n",
    "                    params['7_manufacturer']=g\n",
    "                h=soup.find('span', class_='itemLbl product-option-consist', span='').find_next_sibling('span').text\n",
    "                if h: \n",
    "                    params['4_consistation']=h\n",
    "                i=soup.find('div', class_='product-desc', p='').text\n",
    "                if i: \n",
    "                    params['93_description']=i\n",
    "                products.append(params)\n",
    "            except:\n",
    "                products.append(params)\n",
    "                print('Часть тегов не обнаружена. Двигаюсь дальше')\n",
    "                pass\n",
    "        else:\n",
    "            print('Проблемы с супом')\n",
    "            save_product_info(products) # промежуточное сохранение на случай бана\n",
    "            return False\n",
    "    save_product_info(products) # для сохранения в CSV\n",
    "    #save_to_sqlite(database_path, products) # раскоментить для сохранения в SQLite базу\n",
    "    #return products\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВЕРСИЯ 2 С ПРОМЕЖУТОЧНЫМ СОХРАНЕНИЕМ\n",
    "def get_info2(headers, filename):\n",
    "    global df\n",
    "    results = open_csv(filename)\n",
    "    queue = deque()\n",
    "    queue.extend(results)\n",
    "    visited = get_visited_urls()[0]\n",
    "    df = get_visited_urls()[1]\n",
    "    session = requests.Session()\n",
    "    products = [] \n",
    "    #for url in queue:\n",
    "    #for url in tqdm(queue, desc='Getting info from product page url', leave=False): #experimental tooling \n",
    "    while tqdm(queue, desc='Getting info from product page url', leave=False): #experimental tooling    \n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue   \n",
    "        params={} \n",
    "        r = session.get(url, headers=headers)\n",
    "        soup = bs(r.text, 'lxml')\n",
    "        if soup:\n",
    "            visited.add(url) \n",
    "            try:\n",
    "                params['91_url']=url\n",
    "                j=soup.find('div', class_='my_recipe_image-box').find_next('img')['src']\n",
    "                if j:\n",
    "                    params['92_image']=j\n",
    "                a=soup.find('span', class_='product-title').text\n",
    "                if a:\n",
    "                    params['1_title']=a\n",
    "                i=soup.find('div', class_='product-desc', p='').text\n",
    "                if i: \n",
    "                    params['93_description']=i\n",
    "                b=soup.find('span', style='white-space:nowrap;').text\n",
    "                if b:\n",
    "                    params['2_articul']=b.strip()\n",
    "                c=soup.find('span', class_='this_product_price itemPrice').text.lstrip('Цена:').strip()\n",
    "                if c:\n",
    "                    params['3_price']=c \n",
    "                d=soup.find('span', class_='itemNal product-option-isset').text    \n",
    "                if d: \n",
    "                    params['8_stock']=d\n",
    "                e=soup.find('span', class_='itemLbl product-option-weight', span='').find_next_sibling('span').text.strip()\n",
    "                if e: \n",
    "                    params['5_weight/volume']=e\n",
    "                f=soup.find('div', class_='itemVal', span='').text.lstrip('Объем:').strip()\n",
    "                #if f: \n",
    "                    #params['6_volume/weight']=f\n",
    "               #g=soup.find('span', class_='itemLbl product-option-manufacturer', span='').find_next_sibling('span').text\n",
    "                if g: \n",
    "                    params['7_manufacturer']=g\n",
    "                h=soup.find('span', class_='itemLbl product-option-consist', span='').find_next_sibling('span').text\n",
    "                if h: \n",
    "                    params['4_consistation']=h\n",
    "                i=soup.find('div', class_='product-desc', p='').text\n",
    "                if i: \n",
    "                    params['93_description']=i\n",
    "                products.append(params)\n",
    "            except:\n",
    "                products.append(params)\n",
    "                print('Часть тегов не обнаружена. Двигаюсь дальше')\n",
    "                pass\n",
    "        else:\n",
    "            print('Проблемы с супом')\n",
    "            save_product_info(products)\n",
    "            return False\n",
    "    save_product_info(products)\n",
    "    add_previous_part(df) \n",
    "    #save_to_sqlite(database_path, products) # для сохранения в SQLite базу\n",
    "    #return products # только для теста sql3\n",
    "    print('DONE!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВЕРСИЯ 3 С ПРОМЕЖУТОЧНЫМ СОХРАНЕНИЕМ И ОБРАБОТКОЙ ЭКСЕПШЕНОВ\n",
    "def get_info3(headers, filename):\n",
    "    ConnectErrs  = (TimeoutError, ConnectionError)\n",
    "    global df\n",
    "    results = open_csv(filename)\n",
    "    queue = deque()\n",
    "    queue.extend(results)\n",
    "    visited = get_visited_urls()[0]\n",
    "    df = get_visited_urls()[1]\n",
    "    session = requests.Session()\n",
    "    products = []\n",
    "    Count=0\n",
    "    #for url in queue:\n",
    "    #for url in tqdm(queue, desc='Getting info from product page url', leave=False): #не работает с изменяемой очередью\n",
    "    #while tqdm(queue, desc='Getting info from product page url', leave=True): # отключил т.к выглядит жутко\n",
    "    while queue:\n",
    "        Count=Count+1\n",
    "        msg = f'Обрабатываю ссылку № {Count}'\n",
    "        print(msg)\n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue # начнет цикл while заново     \n",
    "        params={} \n",
    "        try:\n",
    "            r = session.get(url, headers=headers)\n",
    "            soup = bs(r.text, 'lxml')\n",
    "            if soup:\n",
    "                visited.add(url) #добавляем в посещенные, только когда получим суп. т.е. страница доступна\n",
    "                try:\n",
    "                    params['91_url']=url\n",
    "                    j=soup.find('div', class_='my_recipe_image-box').find_next('img')['src']\n",
    "                    if j:\n",
    "                        params['92_image']=j\n",
    "                    a=soup.find('span', class_='product-title').text\n",
    "                    if a:\n",
    "                        params['1_title']=a\n",
    "                    i=soup.find('div', class_='product-desc', p='').text\n",
    "                    if i: \n",
    "                        params['93_description']=i\n",
    "                    b=soup.find('span', style='white-space:nowrap;').text\n",
    "                    if b:\n",
    "                        params['2_articul']=b.strip()\n",
    "                    c=soup.find('span', class_='this_product_price itemPrice').text.lstrip('Цена:').strip()\n",
    "                    if c:\n",
    "                        params['3_price']=c \n",
    "                    d=soup.find('span', class_='itemNal product-option-isset').text    \n",
    "                    if d: \n",
    "                        params['8_stock']=d\n",
    "                    e=soup.find('span', class_='itemLbl product-option-weight', span='').find_next_sibling('span').text.strip()\n",
    "                    if e: \n",
    "                        params['5_weight/volume']=e\n",
    "                    f=soup.find('div', class_='itemVal', span='').text.lstrip('Объем:').strip()\n",
    "                    #if f: \n",
    "                        #params['6_volume/weight']=f\n",
    "                   #g=soup.find('span', class_='itemLbl product-option-manufacturer', span='').find_next_sibling('span').text\n",
    "                    if g: \n",
    "                        params['7_manufacturer']=g\n",
    "                    h=soup.find('span', class_='itemLbl product-option-consist', span='').find_next_sibling('span').text\n",
    "                    if h: \n",
    "                        params['4_consistation']=h\n",
    "                    i=soup.find('div', class_='product-desc', p='').text\n",
    "                    if i: \n",
    "                        params['93_description']=i\n",
    "                    products.append(params)\n",
    "                except:\n",
    "                    products.append(params)\n",
    "                    print('Часть тегов не обнаружена. Двигаюсь дальше\\n')\n",
    "                    pass\n",
    "            else:\n",
    "                print('Проблемы с супом')\n",
    "                save_product_info(products)# промежуточное сохранение на случай если суп не обнаружен\n",
    "                return False\n",
    "        except ConnectErrs :\n",
    "            save_product_info(products) #запуск в случае срабатывания эксепшена\n",
    "            add_previous_part(df) #запуск в случае срабатывания эксепшена\n",
    "            print('Аварийное завершение работы')\n",
    "    save_product_info(products) #  для сохранения в CSV\n",
    "    add_previous_part(df) # перезапись предыдущих результатов в случае перезапуска парсера\n",
    "    print('DONE!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запуск сохранения в SQL3-базу\n",
    "save_to_sqlite(database_path, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = '../SQL-light/kotelock.db3'\n",
    "#rows = get_info(headers, filename)\n",
    "#sqlite_insert = \"INSERT OR REPLACE INTO kotelock_tab (articul,title,price,weight,stock,url,image,description) VALUES (?,?,?,?,?,?,?,?)\"\n",
    "sqlite_insert = \"INSERT INTO kotelock_tab (articul,title,price,weight,stock,url,image,description) VALUES (?,?,?,?,?,?,?,?)\"\n",
    "\n",
    "\n",
    "#F0 - main block to save in SQL3\n",
    "def save_to_sqlite(database_path, rows):\n",
    "    global connection\n",
    "    connection = __connect(database_path) #F1\n",
    "    __create_table() #F2\n",
    "    for row in rows:\n",
    "        __save_row(row) #F3 row - это 1 строка нашего словарика\n",
    "    __close_connection() #F4\n",
    "    \n",
    "#F1\n",
    "def __connect(database):\n",
    "    conn = sqlite3.connect(database).cursor()  \n",
    "    return conn\n",
    "\n",
    "#F2\n",
    "def __create_table():\n",
    "    connection.execute(\"\"\"CREATE TABLE IF NOT EXISTS kotelock_tab (\n",
    "                                articul TEXT PRIMARY KEY,\n",
    "                                title TEXT NOT NULL,\n",
    "                                price TEXT,\n",
    "                                weight TEXT,\n",
    "                                stock TEXT,\n",
    "                                url TEXT,\n",
    "                                image TEXT,\n",
    "                                description TEXT)\n",
    "                        \"\"\")\n",
    "\n",
    "#F3\n",
    "def __save_row(row):\n",
    "    connection.execute(sqlite_insert, (\n",
    "    row.get('2_articul'),    \n",
    "    row.get('1_title'),\n",
    "    row.get('3_price'),\n",
    "    row.get('5_weight/volume'),\n",
    "    row.get('8_stock'),\n",
    "    row.get('91_url'),\n",
    "    row.get('92_image'),\n",
    "    row.get('93_description')\n",
    "    ))\n",
    "    \n",
    "#F4\n",
    "def __close_connection():\n",
    "    if connection:\n",
    "            connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
