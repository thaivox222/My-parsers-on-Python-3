{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собрано вакансий: 19\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from  bs4 import BeautifulSoup as bs\n",
    "import csv # для сохранениея результатов\n",
    "\n",
    "headers = {'accept': '*/*',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/'\n",
    "          }\n",
    "url='https://bryansk.hh.ru/search/vacancy?area=1&clusters=true&search_period=1&text=TeamLead&page=0'\n",
    "hh_parse(url, headers)#запускаем функцию - не забудь поменять имя парсера!!!\n",
    "\n",
    "\n",
    "def hh_parse(url, headers):\n",
    "    jobs=[]\n",
    "    session = requests.Session() #это эмуляция работы одного посетителя сайта в рамках одной сессии\n",
    "    request = session.get(url, headers=headers)\n",
    "    if request.status_code == 200: # парсеры включаются в работу после получения 200 кода ответа от сервера\n",
    "        #start = time.time() #засекаем время работы парсера\n",
    "        #print('OK')\n",
    "        soup = bs(request.content, 'html.parser') # вариант с простым парсером\n",
    "        divs=soup.find_all('div', attrs={'data-qa': 'vacancy-serp__vacancy'})\n",
    "        #divs=soup.find_all('div', attrs={'class': 'vacancy-serp'})\n",
    "        for div in divs:\n",
    "            title = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-title'}).text #чтобы не ссылки, а названия вакансий выдал нам\n",
    "            hrefs = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-title'})['href']\n",
    "            company = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-employer'}).text\n",
    "            text1 = div.find('div', attrs={'data-qa': 'vacancy-serp__vacancy_snippet_responsibility'}).text\n",
    "            text2 = div.find('div', attrs={'data-qa': 'vacancy-serp__vacancy_snippet_requirement'}).text\n",
    "            content = text1+' '+text2\n",
    "            #print(title)#менять принт под каждый итер цикла для теста отработки\n",
    "            jobs.append({\n",
    "                'title': title,\n",
    "                'href': hrefs,\n",
    "                'company': company,\n",
    "                'content': content \n",
    "            })\n",
    "        print (str('Собрано вакансий: ') + str(len(jobs)))\n",
    "    else:\n",
    "        print('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''См. часть 2 для записи результатов в файл'''\n",
    "'''Во второй части надо установить через пип новую библу - lxml (ей пользовался Молчанов)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR or Done200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from  bs4 import BeautifulSoup as bs\n",
    "import csv # для сохранениея результатов\n",
    "\n",
    "headers = {'accept': '*/*',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/'\n",
    "          }\n",
    "url='https://bryansk.hh.ru/search/vacancy?area=1&clusters=true&search_period=1&text=TeamLead&page=0'\n",
    "\n",
    "jobs = wtf_hh(url, headers) #запускаем функцию - не забудь поменять имя парсера!!!\n",
    "files_writer(jobs)# запуск функции записи в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь уже парсим не только первую страницу запроса, а еще и пробежимся по страницам пагинации\n",
    "def wtf_hh(url, headers):\n",
    "    session = requests.Session() #это эмуляция работы одного посетителя сайта в рамках одной сессии\n",
    "    r = session.get(url, headers=headers)\n",
    "    jobs=[]\n",
    "    urls=[] #список куда бум складывать урлы пагинации\n",
    "    urls.append(url)#сразу добавим первую страницу, на тот случай, если она единственная (т.е. нет пагинации)\n",
    "    if r.status_code == 200:# парсеры включаются в работу после получения 200 кода ответа от сервера\n",
    "        #print('OK')\n",
    "        soup = bs(r.content, 'lxml')  # вариант с продвинутым парсером\n",
    "        try:\n",
    "            pagination = soup.find_all('a', attrs={'data-qa': 'pager-page'})#говорим парсеру обнаружить все теги со ссылками на старницы пагинаци\n",
    "            count = int(pagination[-1].text) #счетчик пагинации\n",
    "            for i in range(count):\n",
    "                url = f'https://bryansk.hh.ru/search/vacancy?area=1&clusters=true&search_period=1&text=TeamLead&page={i}' #итератор перебора урлов в пагинации\n",
    "                if url not in urls:\n",
    "                    urls.append(url)   \n",
    "        except:\n",
    "            pass # если не найдет пагинацию, то просто пойдет дальше\n",
    "    for url in urls: # а это уже директива для перебора самих урлов пагинации в  списке\n",
    "        r = session.get(url, headers=headers)\n",
    "        soup = bs(r.content, 'lxml')    \n",
    "        divs=soup.find_all('div', attrs={'data-qa': 'vacancy-serp__vacancy'})\n",
    "        for div in divs:\n",
    "            try: #чтобы не выбивало, если какой-то тег не обнаружен\n",
    "                \n",
    "                title = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-title'}).text #чтобы не ссылки, а названия вакансий выдал нам\n",
    "                hrefs = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-title'})['href']\n",
    "                company = div.find('a', attrs={'data-qa': 'vacancy-serp__vacancy-employer'}).text\n",
    "                text1 = div.find('div', attrs={'data-qa': 'vacancy-serp__vacancy_snippet_responsibility'}).text\n",
    "                text2 = div.find('div', attrs={'data-qa': 'vacancy-serp__vacancy_snippet_requirement'}).text\n",
    "                content = text1+' '+text2\n",
    "                jobs.append({\n",
    "                    'title': title,\n",
    "                    'href': hrefs,\n",
    "                    'company': company,\n",
    "                    'content': content \n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "            #for i in pagination:\n",
    "                #print(pagination[-1].text) #происм вывести номер последней страницы, которая будет парсится\n",
    "            #print(len(jobs))\n",
    "    else:\n",
    "        print('ERROR or Done'+ str(r.status_code))\n",
    "    return (jobs) #если не сделать рутурн, то список jobs так и останется пустым!   \n",
    "\n",
    "        \n",
    "'''А теперь скинем все в csv'''\n",
    "def files_writer(jobs):\n",
    "    with open('python_hh_vacansies.csv', 'w', encoding='utf-8') as  file:\n",
    "        a_pen = csv.writer(file)\n",
    "        a_pen.writerow(('Название вакансии', 'URL', 'Название компании', 'Описание вакансии'))\n",
    "        for job in  jobs:\n",
    "            a_pen.writerow((job['title'],  job['href'], job['company'], job['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Название вакансии</th>\n",
       "      <th>URL</th>\n",
       "      <th>Название компании</th>\n",
       "      <th>Описание вакансии</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TeamLead Android</td>\n",
       "      <td>https://bryansk.hh.ru/vacancy/35851629?query=T...</td>\n",
       "      <td>ООО Криптериум</td>\n",
       "      <td>Руководство отделом. Планирования и решения те...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digital Content Media Team Lead</td>\n",
       "      <td>https://bryansk.hh.ru/vacancy/35792850?query=T...</td>\n",
       "      <td>FOREO Ltd.</td>\n",
       "      <td>Manage content advertising activities for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Team Lead (PHP, JS)</td>\n",
       "      <td>https://bryansk.hh.ru/vacancy/34941067?query=T...</td>\n",
       "      <td>BeeJee</td>\n",
       "      <td>Экспертный уровень PHP-разработки (Yii, CI, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Team lead Python developer</td>\n",
       "      <td>https://bryansk.hh.ru/vacancy/35846186?query=T...</td>\n",
       "      <td>BestDoctor</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User Acquisition Team Lead</td>\n",
       "      <td>https://bryansk.hh.ru/vacancy/35696020?query=T...</td>\n",
       "      <td>ООО G5 Entertainment AB</td>\n",
       "      <td>Организация работы UAM-ов (включая наем и обуч...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Название вакансии  \\\n",
       "0                 TeamLead Android   \n",
       "1  Digital Content Media Team Lead   \n",
       "2              Team Lead (PHP, JS)   \n",
       "3       Team lead Python developer   \n",
       "4       User Acquisition Team Lead   \n",
       "\n",
       "                                                 URL        Название компании  \\\n",
       "0  https://bryansk.hh.ru/vacancy/35851629?query=T...           ООО Криптериум   \n",
       "1  https://bryansk.hh.ru/vacancy/35792850?query=T...               FOREO Ltd.   \n",
       "2  https://bryansk.hh.ru/vacancy/34941067?query=T...                   BeeJee   \n",
       "3  https://bryansk.hh.ru/vacancy/35846186?query=T...               BestDoctor   \n",
       "4  https://bryansk.hh.ru/vacancy/35696020?query=T...  ООО G5 Entertainment AB   \n",
       "\n",
       "                                   Описание вакансии  \n",
       "0  Руководство отделом. Планирования и решения те...  \n",
       "1  Manage content advertising activities for the ...  \n",
       "2   Экспертный уровень PHP-разработки (Yii, CI, K...  \n",
       "3                                                     \n",
       "4  Организация работы UAM-ов (включая наем и обуч...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = jobs\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['Название вакансии', 'URL', 'Название компании', 'Описание вакансии']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Экспортнем все это дело в Эксель!\n",
    "writer = pd.ExcelWriter('HH.xlsx')\n",
    "df.to_excel(writer, 'TTLs')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
