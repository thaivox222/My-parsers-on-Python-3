{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Пример парсера coinmarketcap.com в несколько процессов (multiprocess)\n",
    "Сейчас этот паресер не работает т.к. на сайте висит защита cloudflare от ddos.\n",
    "Но автор говорит, что если подкрутить кое-в-каких местах, то должно прокатывать.\n",
    "Взято в качестве примера по методологии самого парсинга'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "'''API - легальный способ получить данные без парсинга. Так что смотри сначала,\n",
    "может там есть API и выдачи с него тебе будет достаточно?\n",
    "А вообще за такой парсинг могут забанить к хуям, так, что запускать этот код не рекомендую.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подход. Требуется обнаружить ссылки на главной на сами альткоины. На главной также есть пагинация и view all.\n",
    "Затем перейти на страницу альткоина, получить его название и цену. ПОГНАЛИ!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from  bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime #это чтобы засекать время работы парсера над каждоым альткоином\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем точку входа\n",
    "if __main__== '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start = datetime.now() #засекаем время начала парсинга\n",
    "    url = 'https://coinmarketcap.com/alt/views/all'  #- это наша основаня ссылка\n",
    "    all_links=get_all_links(get_html(url))\n",
    "'''Получается такая вот матрешка.\n",
    "1.Передаем url в функцию get_html. На выходе получаем станицу в txt\n",
    "2.Эту страницу уже передаем в фукцию get_all_links. На выходе получаем список с ссылками'''\n",
    "    for index, url in enumerate(all_links): #здесь начинается вызов функций по порядку. Очень важно не заблудится где какой параметр передается.\n",
    "        html = get_html(url)\n",
    "        data = get_page_data(html)\n",
    "        write_csv(data)# получается, что это цикл, каскадно запускающий все эти функции работает внутри самой функции get_all_links\n",
    "        print(index)#выводит порядковый номер монеты\n",
    "    end =  datetime.now() #засекаем время конца парсинга\n",
    "    total = end-start\n",
    "    print(str(total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    r=requests.get(url)\n",
    "    return r.text #из ответа сервера мы хотим получить текст (а точнее это будет код html-страницы в виде текста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот блок исключительно для выцарапывания ссылок. Это еще на сам парсинг как таковой.\n",
    "def get_all_links(html):\n",
    "    soup = BeautifulSoup(html, 'lxml') # lxml - это парсер, кот. будет  использоваться\n",
    "    tds = soup.find('table', id = 'currencies-all').find_all('td', _class='currency-name') #table - это имя тега, а id - его атрибут со значением\n",
    "#вспоминаем. что find - это найти первое попавшееся значение, а find_all - все значения по заданному условию\n",
    "    links - []\n",
    "    for td in tds:\n",
    "        a = td.find('a').get('href')# в объекте супа td находим тег a с атрибутом href.Это будет урл ссылки (без домена)\n",
    "        domen ='https://coinmarketcap.com/' #а это сам домен с которым будет склеивать урл\n",
    "        link = domen + a #конкатенация\n",
    "        links.append(link) # Положили ссылочку в список links\n",
    "    return links# на выходе работы функции получаем список со ссылками\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А вот здесь уже начинается сам парсинг. Вытягиваем название альткоина и его цену\n",
    "def get_page_data(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    try:\n",
    "        name = soup.find('h1', _class='text-large').text.strip() # основная часть кода\n",
    "    except:\n",
    "        name='' # страховочная часть на случай исключения\n",
    "    try:\n",
    "        price = soup.find('span', id='quote_price').text.strip()\n",
    "    except:\n",
    "        price=''\n",
    "    data = {'name': name, 'price': price}\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В этом блоке запишем все в csv. Заметь, что для этого нам не надо\n",
    "#импортировать всю Панду - достаточно встроенного метода csv. смотри в главном модуле\n",
    "def write_csv(data):\n",
    "    with open('coinmarketcap.csv', 'a') as f:# а - это значит добавить в конец. Легко запомнить. т.к. это сокращение от append\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow( (data['name'],\n",
    "                              ['price']))#тут как бы говорим, что надо записать в строку наш словарик имя-цена\n",
    "#функция ничего не возвращает, а только записывает (да, так тоже можно)\n",
    "        print(data['name'], 'parsed')# ну это не обязательно, так. чисто для контроля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Это тот же самый скрипт, но он уже сделанный в многопоточном варианте '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from  bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime #это чтобы засекать время работы парсера над каждым альткоином\n",
    "from multiprocessing import Pool #это чтобы парсить в несколько потоков\n",
    "\n",
    "# Создаем точку входа (она вроде как и не пригодилась нигде)\n",
    "if __main__== '__main__':\n",
    "    main()\n",
    "    \n",
    "def main():\n",
    "    start = datetime.now() #засекаем время начала парсинга\n",
    "    url = 'https://coinmarketcap.com/alt/views/all'  #- это наша основаня ссылка\n",
    "    all_links=get_all_links(get_html(url))\n",
    "'''Получается такая вот матрешка.\n",
    "1.Передаем url в функцию get_html. На выходе получаем станицу в txt\n",
    "2.Эту страницу уже передаем в фукцию get_all_links. На выходе получаем список с ссылками'''\n",
    "    #for index, url in enumerate(all_links): #здесь начинается вызов функций по порядку. Очень важно не заблудится где какой параметр передается.\n",
    "        #html = get_html(url)\n",
    "        #data = get_page_data(html)\n",
    "        #write_csv(data)# получается, что это цикл, каскадно запускающий все эти функции работает внутри самой функции get_all_links\n",
    "        #print(index)#выводит порядковый номер монеты\n",
    "    with Pool(40) as p: #40-это кол-во процессов python. По идее их столько будет в диспетчере задач\n",
    "        p.map(make_all, all_links) #make_all пишется без скобок т.к. здесь не результат работы функции, а сама функция\n",
    "         \n",
    "    end =  datetime.now() #засекаем время конца парсинга\n",
    "    total = end-start\n",
    "    print(str(total))\n",
    "    \n",
    "def make_all(url): #Здесь сборка всех функций\n",
    "        html = get_html(url)\n",
    "        data = get_page_data(html)\n",
    "        write_csv(data)# получается, что это цикл, каскадно запускающий все эти функции работает внутри самой функции get_all_links\n",
    "    \n",
    "    \n",
    "\n",
    "def get_html(url):\n",
    "    r=requests.get(url)\n",
    "    return r.text #из ответа сервера мы хотим получить текст (а точнее это будет код html-страницы в виде текста)\n",
    "\n",
    "# Этот блок исключительно для выцарапывания ссылок. Это еще на сам парсинг как таковой.\n",
    "def get_all_links(html):\n",
    "    soup = BeautifulSoup(html, 'lxml') # lxml - это парсер, кот. будет  использоваться\n",
    "    tds = soup.find('table', id = 'currencies-all').find_all('td', _class='currency-name') #table - это имя тега, а id - его атрибут со значением\n",
    "#вспоминаем. что find - это найти первое попавшееся значение, а find_all - все значения по заданному условию\n",
    "    links - []\n",
    "    for td in tds:\n",
    "        a = td.find('a').get('href')# в объекте супа td находим тег a с атрибутом href.Это будет урл ссылки (без домена)\n",
    "        domen ='https://coinmarketcap.com/' #а это сам домен с которым будет склеивать урл\n",
    "        link = domen + a #конкатенация\n",
    "        links.append(link) # Положили ссылочку в список links\n",
    "    return links# на выходе работы функции получаем список со ссылками\n",
    "\n",
    "# А вот здесь уже начинается сам парсинг. Вытягиваем название альткоина и его цену\n",
    "def get_page_data(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    try:\n",
    "        name = soup.find('h1', _class='text-large').text.strip() # основная часть кода\n",
    "    except:\n",
    "        name='' # страховочная часть на случай исключения\n",
    "    try:\n",
    "        price = soup.find('span', id='quote_price').text.strip()\n",
    "    except:\n",
    "        price=''\n",
    "    data = {'name': name, 'price': price}\n",
    "    return data\n",
    "\n",
    "# В этом блоке запишем все в csv. Заметь, что для этого нам не надо\n",
    "#импортировать всю Панду - достаточно встроенного метода csv. смотрии в главном модуле\n",
    "def write_csv(data):\n",
    "    with open('coinmarketcap.csv', 'a') as f:# а - это значит добавить в конец. Легко запомнить. т.к. это сокращение от append\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow( (data['name'],\n",
    "                              ['price']))#тут как бы говорим, что надо записать в строку наш словарик имя-цена\n",
    "#функция ничего не возвращает, а только записывает (да, так тоже можно)\n",
    "        print(data['name'], 'parsed')# ну это не обязательно, так. чисто для контроля\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Обрати внимание, что весь парсер целиком состоит из функций, которые вызывают друг-друга '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
